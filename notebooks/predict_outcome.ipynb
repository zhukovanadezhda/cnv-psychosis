{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of the outcome (converters vs non-converters) based on CNV parameters\n",
    "\n",
    "Two predictive models were developed to distinguish between converters (n=302) and non-converters (n=75) based on a set of CNV parameters. The models chosen for this analysis were logistic regression and random forest. Logistic regression was selected due to its simplicity, interpretability, and effectiveness in binary classification tasks, making it suitable for identifying key predictors that distinguish between the two groups. Random forest was chosen because of its ability to capture complex interactions between features, and its robustness against overfitting, which makes it suitable for exploring more complex relationships within the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer, f1_score, precision_score, recall_score, roc_auc_score, roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbalancedPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('../results/merged_per_individual_annotations.csv')\n",
    "\n",
    "# Remove outliers with ID S36958 and S36981\n",
    "data = data[data['ID'] != 'S36958']\n",
    "data = data[data['ID'] != 'S36981']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the index\n",
    "data.index = data['ID']\n",
    "\n",
    "# Convert the phenotype to binary\n",
    "data['Phenotype_2'] = data['Phenotype_2'].map({'non-converter': 0, 'converter': 1})\n",
    "\n",
    "# Drop the rows with missing values\n",
    "data.dropna(subset=['Phenotype_2'], inplace=True)\n",
    "\n",
    "# Create the predictors and target\n",
    "predictors = ['Avg_copy_number_DEL', 'Avg_copy_number_DUP', \n",
    "              'Brain_rare_length_DEL', 'Brain_rare_length_DUP',\n",
    "              'Long_brain_rare_length_DEL', 'Long_brain_rare_length_DUP',\n",
    "              'Long_rare_length_DEL', 'Long_rare_length_DUP',\n",
    "              'NB_brain_genes_rare_DEL', 'NB_brain_genes_rare_DUP',\n",
    "              'NB_brain_rare_DEL', 'NB_brain_rare_DUP', 'NB_genes_rare_DEL',\n",
    "              'NB_genes_rare_DUP', 'NB_long_brain_rare_DEL', \n",
    "              'NB_long_brain_rare_DUP', 'NB_long_rare_DEL', \n",
    "              'NB_long_rare_DUP', 'NB_rare_DEL', 'NB_rare_DUP', \n",
    "              'Rare_length_DEL', 'Rare_length_DUP']\n",
    "target = 'Phenotype_2'\n",
    "\n",
    "# Create the X and y\n",
    "X = data[predictors]\n",
    "y = data[target]\n",
    "\n",
    "# Set inf values to 0 \n",
    "inf_mask = np.isinf(X[['Avg_copy_number_DEL']])\n",
    "X.loc[inf_mask['Avg_copy_number_DEL'], 'Avg_copy_number_DEL'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustrate the data imbalance\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the imbalanced nature of the dataset (with a significantly higher number of converters compared to non-converters), specific strategies were employed to ensure robust model performance and avoid bias towards the majority class. Cross-validation with class weights was utilized to address this imbalance. In the logistic regression model, class weights were adjusted to provide greater emphasis on the minority class (non-converters) during model training, thereby ensuring that the predictive models accurately identified instances of both converters and non-converters. For the random forest model, the class weights were similarly adjusted to enhance sensitivity towards the minority class.  \n",
    "\n",
    "To effectively validate the model’s performance and prevent overfitting, a stratified cross-validation approach was employed with 70% of the data allocated for training and 30% for testing. Stratified cross-validation was chosen to maintain the original class distribution in each fold, ensuring that each fold had a representative sample of both converters and non-converters. This approach provided a more reliable evaluation of the model's predictive power across different subsets of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and configure models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(class_weight='balanced',\n",
    "                                              max_iter=1000, \n",
    "                                              random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(class_weight='balanced',\n",
    "                                            n_estimators=100, \n",
    "                                            random_state=42)\n",
    "}\n",
    "\n",
    "# Create pipelines for each model\n",
    "pipelines = {\n",
    "    name: ImbalancedPipeline(steps=[\n",
    "        ('smote', SMOTE(sampling_strategy='minority', random_state=42)),\n",
    "        (name.lower().replace(' ', '_'), model)\n",
    "    ]) for name, model in models.items()\n",
    "}\n",
    "\n",
    "# Define cross-validation strategy with stratified splits\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define metrics for evaluation\n",
    "scoring = {\n",
    "    'f1': make_scorer(f1_score),\n",
    "    'precision': make_scorer(precision_score),\n",
    "    'recall': make_scorer(recall_score),\n",
    "    'roc_auc': make_scorer(roc_auc_score, needs_proba=True) \n",
    "}\n",
    "\n",
    "\n",
    "def evaluate_pipeline(pipeline, X, y):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of a pipeline using cross-validation and return\n",
    "    the metrics summary.\n",
    "\n",
    "    Parameters:\n",
    "    - pipeline: the pipeline object to evaluate\n",
    "    - X: feature matrix\n",
    "    - y: target vector\n",
    "\n",
    "    Returns:\n",
    "    - metrics_summary: a dictionary with mean and variance for each metric\n",
    "    \"\"\"\n",
    "    # Perform cross-validation and collect results\n",
    "    results = cross_validate(pipeline, X, y, \n",
    "                             cv=skf, \n",
    "                             scoring=scoring, \n",
    "                             return_train_score=False)\n",
    "    # Summarize the results for each metric\n",
    "    metrics_summary = {metric: {'mean': results[f'test_{metric}'].mean(),\n",
    "                                'variance': results[f'test_{metric}'].var()} \n",
    "                       for metric in scoring}\n",
    "    return metrics_summary\n",
    "\n",
    "\n",
    "def create_performance_table(models, X, y):\n",
    "    \"\"\"\n",
    "    Create a DataFrame summarizing the performance metrics for each model.\n",
    "\n",
    "    Parameters:\n",
    "    - models: a dictionary of model names and their corresponding pipelines\n",
    "    - X: feature matrix\n",
    "    - y: target vector\n",
    "\n",
    "    Returns:\n",
    "    - performance_table: DataFrame with performance metrics for each model\n",
    "    \"\"\"\n",
    "    performance_data = []\n",
    "    # Evaluate each model and collect performance metrics\n",
    "    for name, pipeline in models.items():\n",
    "        metrics_summary = evaluate_pipeline(pipeline, X, y)\n",
    "        performance_data.append({\n",
    "            'Model': name,\n",
    "            'F1 Mean': metrics_summary['f1']['mean'],\n",
    "            'F1 Variance': metrics_summary['f1']['variance'],\n",
    "            'Precision Mean': metrics_summary['precision']['mean'],\n",
    "            'Precision Variance': metrics_summary['precision']['variance'],\n",
    "            'Recall Mean': metrics_summary['recall']['mean'],\n",
    "            'Recall Variance': metrics_summary['recall']['variance'],\n",
    "            'ROC AUC Mean': metrics_summary['roc_auc']['mean'],\n",
    "            'ROC AUC Variance': metrics_summary['roc_auc']['variance']\n",
    "        })\n",
    "    # Convert the performance data into a DataFrame\n",
    "    return pd.DataFrame(performance_data)\n",
    "\n",
    "\n",
    "def plot_mean_roc(ax, pipeline, X, y, title):\n",
    "    \"\"\"\n",
    "    Plot the mean ROC curve with shaded area for standard deviation.\n",
    "\n",
    "    Parameters:\n",
    "    - ax: the matplotlib axis to plot on\n",
    "    - pipeline: the pipeline object to evaluate\n",
    "    - X: feature matrix\n",
    "    - y: target vector\n",
    "    - title: title for the plot\n",
    "    \"\"\"\n",
    "    tprs = []\n",
    "    mean_fpr = np.linspace(0, 1, 100) \n",
    "\n",
    "    # Compute ROC curve for each fold\n",
    "    for train_idx, test_idx in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        # Fit the pipeline on the training data\n",
    "        pipeline.fit(X_train, y_train)  \n",
    "        # Get probability predictions\n",
    "        y_prob = pipeline.predict_proba(X_test)[:, 1]  \n",
    "\n",
    "        # Compute ROC curve for the current fold\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_prob)  \n",
    "        # Interpolate to ensure all ROC curves are plotted on the same x-axis\n",
    "        tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "        tprs[-1][0] = 0.0  # Fix the beginning of the curve\n",
    "\n",
    "        # Plot each fold's ROC curve with transparency\n",
    "        ax.plot(fpr, tpr, color='grey', lw=1.5, alpha=0.4)\n",
    "\n",
    "    # Compute mean and standard deviation of TPR\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "\n",
    "    # Plot mean ROC curve\n",
    "    ax.plot(mean_fpr, mean_tpr, color='blue', lw=3, alpha=0.5,\n",
    "            label='Mean ROC (AUC = {:.2f})'.format(auc(mean_fpr, mean_tpr)))\n",
    "    # Fill the area between the standard deviation bounds\n",
    "    ax.fill_between(mean_fpr, np.clip(mean_tpr - std_tpr, 0, 1), \n",
    "                    np.clip(mean_tpr + std_tpr, 0, 1),\n",
    "                    color='blue', alpha=0.2, label='± std. dev.')\n",
    "\n",
    "    ax.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title(title)\n",
    "    ax.legend(loc='lower right')\n",
    "\n",
    "\n",
    "def main(X, y):\n",
    "    \"\"\"\n",
    "    Evaluate models, plot ROC curves, and display feature importance.\n",
    "\n",
    "    Parameters:\n",
    "    - X: feature matrix\n",
    "    - y: target vector\n",
    "    \"\"\"\n",
    "    # Check if X is a DataFrame to extract feature names\n",
    "    feature_names = X.columns if isinstance(X, pd.DataFrame) else None\n",
    "\n",
    "    # Create and display performance table\n",
    "    performance_table = create_performance_table(pipelines, X, y)\n",
    "    print(\"Model Performance Summary:\")\n",
    "    print(performance_table)\n",
    "    performance_table.to_csv(\"../results/model_performances.csv\")\n",
    "\n",
    "    # Plot ROC curves for each model\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "    plot_mean_roc(ax1, pipelines['Logistic Regression'], X, y, \n",
    "                  'ROC Curves for Logistic Regression')\n",
    "    plot_mean_roc(ax2, pipelines['Random Forest'], X, y, \n",
    "                  'ROC Curves for Random Forest')\n",
    "    plt.savefig(\"../graphs/roc_curve.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Feature importances for Random Forest\n",
    "    if 'Random Forest' in pipelines:\n",
    "        rf_importances = (\n",
    "            pipelines['Random Forest'].named_steps['random_forest']\n",
    "            .feature_importances_\n",
    "        )\n",
    "        if feature_names is not None:\n",
    "            # Use feature names if available\n",
    "            feature_importance = pd.Series(\n",
    "                rf_importances, index=feature_names\n",
    "            ).sort_values(ascending=False)\n",
    "        else:\n",
    "            # Default to generic feature names if feature names unavailable\n",
    "            feature_importance = pd.Series(\n",
    "                rf_importances\n",
    "            ).sort_values(ascending=False)\n",
    "        \n",
    "        print(\"Feature Importances for Random Forest:\")\n",
    "        print(feature_importance)\n",
    "\n",
    "main(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both predictive models - logistic regression and random forest - showed performances only slightly better than random guessing, achieving a ROC-AUC of 0.49 and 0.52 respectively. This result suggests that the models struggled to effectively differentiate between converters and non-converters. This outcome was anticipated for several reasons.\n",
    "\n",
    "First, the complex and multifactorial nature of psychosis cannot be fully captured by the current set of features used in these models. The analysis was based solely on genomic data, specifically CNV parameters, without considering other genetic modifications such as single nucleotide variants (SNVs) or tandem repeats. Psychosis is influenced by a complex interplay of genetic, environmental, and neurobiological factors, many of which will be integrated into future datasets to provide a more holistic understanding.  \n",
    "\n",
    "Second, class imbalance in the dataset likely played a role in these results. Although techniques like class weighting and stratification were used to address the imbalance, the models may still have been biased towards the majority class (converters), resulting in poor detection of non-converters.\n",
    "These results underscore the importance of the project's future direction, which involves integrating additional data sources. Incorporating data from various modalities, such as clinical information, other genomic markers, microRNA profiles, and methylation data, could enhance the models' performance by offering a more comprehensive and accurate representation of the factors predicting psychosis.\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "everything",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
